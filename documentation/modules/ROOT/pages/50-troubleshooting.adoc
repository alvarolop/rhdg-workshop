= Troubleshooting
include::_attributes.adoc[]

As we move forward to our final phase, we enter the Troubleshooting Section. Here, we provide you with essential tools and techniques to tackle any issues that come your way. As an administrator of the infrastructure, you'll master the art of collecting cluster information for effective debugging. Join us in this enlightening section to strengthen your troubleshooting skills and enhance your cache optimization abilities.

Let's start by collecting important details about the Openshift setup, including CRDs, and pod logs and events. Then, we'll dig into examining the Data Grid server, getting the caches' configuration and a complete server report. Lastly, we'll focus on JVM debugging, getting info from the Java Heap Dump and Java Thread Dump.


== Collecting Openshift resources

Ok, we don't know yet what is going on. This section will help us to identify all the components of our installation and give us a methodic way of examining issues.

=== Retrieving the list of server pods

This might seem trivial, but we need to list the pods to then execute other commands inside the pods. Execute the following command to retrieve the full list of pods:

.Get the full list of pods
[.console-input]
[source, bash]
----
oc get pods -l user=$YOUR_USER
----

.Output
[.console-output]
[source, bash]
----
NAME        READY   STATUS    RESTARTS   AGE
cluster-0   1/1     Running   0          19m
cluster-1   1/1     Running   0          19m
----


=== Retrieving all the logs for investigation

Here you have to take into account that three main components take part in the configuration and you should know their responsibilities:

* `infinispan-operator-controller-manager`: This is the operator itself and runs in this workshop in a different namespace named `rhdg-operator`. It synchronizes the configuration of the CRDs with Openshift objects and the Infinispan server itself. There is an exception for the Cache CRD when the `.spec.configListener.enabled` = true in the Infinispan CRD - which is enabled by default-, this sync is delegated to the config-listener deployed for each cluster.
+
.Check the logs of the operator pod
[.console-input]
[source, bash]
----
oc logs -f $(oc get pods -n rhdg-operator --template="{{(index .items 0).metadata.name }}") -n rhdg-operator
----
+
* `cluster-config-listener`: There is one per cluster and it can be disabled when `.spec.configListener.enabled` = false in the Infinispan CRD. It is responsible for the bidirectional reconciliation of cache configurations between the Data Grid server and the Cache CRD. 
+
.Check the logs of the config-listener pod
[.console-input]
[source, bash]
----
oc logs -f $(oc get pods -l app=infinispan-config-listener-pod --template="{{(index .items 0).metadata.name }}")
----
+
* `cluster-pods`: These are the actual pods of the cluster. In its logs, you can see issues related to server configuration, the clustering based on JGroups, and access logs.
+
.Check the logs of the server pods
[.console-input]
[source, bash]
----
oc logs -f cluster-0
----




=== Retrieving the Openshift configuration

This information is essential for the Data Grid support team to have a clear idea of the issue that you are facing. The recommended way to collect information is using the inspect command:


.Get the full report of the namespace
[.console-input]
[source, bash]
----
oc adm inspect namespace/$YOUR_USER-rhdg
----

If you don't have enough permissions to execute the above command, you will need that information manually.





== The Data Grid server report

Data Grid Server provides aggregated reports in tar.gz archives that contain diagnostic information about server instances and host systems. The report provides details about CPU, memory, open files, network sockets and routing, and threads, in addition to configuration and log files.

Retrieving the server report takes several steps, as you have to use the Infinispan CLI. In the following command, I have automated the process so that you can generate it and download it easily:

.Get the server report
[.console-input]
[source, bash]
----
# Generate the report
oc exec cluster-0 -- bash -c 'echo "server report" | ./bin/cli.sh --trustall -c https://admin:password@$HOSTNAME:11222 -f -'
# Download the report
oc exec cluster-0 -- bash -c 'files=( *tar.gz* ); cat "${files[0]}"' > $(date +"%Y-%m-%d-%H-%M")-cluster-0-report.tar.gz
----

The report comprises the execution of several useful commands that will help you understand the reasons why the server might be misconfigured:

.Server report output
[.console-output]
[source, bash]
----
$ ls -1 cluster-0-report
    184
    conf
    cpuinfo
    data
    df
    ip-address
    ip-maddress
    ip-mroute
    ip-route
    log
    lsof
    meminfo
    os-release
    ss-tcp
    ss-udp
    uname
----





== Using the REST API to retrieve information from the cluster

This section compiles a set of REST endpoints useful to retrieve configuration from the server.

TIP: Some of the objects of this section might be new to you: Proto files, server tasks, indexes, etc. Do not worry, these commands will be useful for you for the Advanced Exercises Section.

=== Problems with Protobuf? 


You can list all the `.proto` files using the REST API.


.Curl command to retrieve the Proto files
[.console-input]
[source, bash]
----
curl 
----


=== Problems with cache definitions? 

Retrieve the list of caches, its configuration and its status (Healthy or not).


=== Problems with Server tasks? 

List them using REST.

=== Problems with the content of a cache?
Empty the cache using REST.

=== Problems with indexes?

Reindex a cache.



== Converting Cache definitions using the REST API

https://github.com/alvarolop/rhdg8-server#annex-convert-cache-configurations




== JVM debugging


=== Problems with the JVM memory on cluster pods?


* Java Thread Dump
* Java Heap Dump


=== Reached file limit descriptors on a pod?

