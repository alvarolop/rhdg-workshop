= Practical Application
include::_attributes.adoc[]



As we wrap up this workshop series, it's your turn to explore exercises that catch your attention. I recommend starting with *Level I*. Here, you'll dive into monitoring capabilities, set up server pod affinity and anti-affinity within the cluster, and master the art of deploying other clusters in versions not supported by the operator resources.

Moving on to *Level II*, you'll delve into the realm of Proto caches, learning how to interact with them using various data formats. Next up, you'll transition into LDAP-based authorization, where user management shifts from an OCP secret to an external LDAP system. And that's not allâ€”you'll also master the art of configuring a datasource to persist cache entries in a SQL database.


As you reach *Level III*, the focus shifts towards controlling cache access through RBAC. This advanced skillset empowers you to grant cache access to specific users, based on defined roles. But that's not the end of the journey. You'll also harness the cross-site replication feature, as you deploy a second Data Grid cluster. Get ready to enable cache replication through cross-site replication, facilitating seamless data sharing between clusters.



== Level I


=== 1.1. Affinity and anti-affinity

Kubernetes includes anti-affinity capabilities that protect workloads from single points of failure. Anti-affinity works by distributing Data Grid nodes across OpenShift nodes, ensuring that your Data Grid clusters remain available even if hardware failures occur.

The Data Grid operator provides three different settings regarding pod distribution on OpenShift:

* *nodeAffinity*: Describes node affinity scheduling rules for the pod (e.g. avoid placing pods in nodes
that are not intended for Data Grid).
* *podAffinity*: Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone,
etc. as some other pod(s)).
* *podAntiAffinity*: Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same
node, zone, etc. as some other pod(s)).


.*Now it's your turn!*
====
The Openshift cluster for this workshop is pretty simple, so there aren't many configurations that you can do in this regard here. Anyway, we have three zones for the worker nodes:

* `topology.kubernetes.io/zone: eu-west-3a`.
* `topology.kubernetes.io/zone: eu-west-3b`.
* `topology.kubernetes.io/zone: eu-west-3c`.

Configure your Infinispan CR to deploy your cluster with affinity to one zone. Please, in order to spread the pods homogeneously, use the zone that corresponds to the modulus of your user number divided by 3. For example: 

* zone A: user1, user4, user7...
* zone B: user2, user5, user8...
* zone C: user3, user6, user9...

If you want to try another configuration, there is one Postgresql pod in each namespace `$YOUR_USERNAME-rhdg` - You will use it later to configure Datasources. Please, configure a `podAffinity` rule so that your Data Grid cluster pods are collocated in the same node.
====


[TIP]
====
If you're interested in learning more, you can find additional information at this https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.4/html-single/data_grid_operator_guide/index#anti-affinity[link].
====






=== 1.2. Prometheus metrics and Grafana.

Data Grid exposes metrics that can be used by Prometheus and Grafana for monitoring and visualizing the cluster state. Data Grid 8 exposes metrics via a `/metrics` endpoint for integration with metrics tooling such as Prometheus. If you want to begin exploring the Data Grid metrics, start enabling metrics with the Infinispan CR annotation:


.Configure Data Grid to send metrics to the user-workload Prometheus
[.console-input]
[source, yaml]
----
apiVersion: infinispan.org/v1
kind: Infinispan
metadata:
  name: cluster
  annotations:
    # Set the monitoring field to true <---
    infinispan.org/monitoring: 'true'
----

Wait for a minute and go to the `Developer Perspective` > `Observe` > `Metrics` tab > `Custom Query`. Query any metric beginning with `vendor_` and you will see your Data Grid cluster pods.


.View metrics of a pod from the OCP web console
image::60-exercises-metrics-dashboard.png[]


.*Now it's your turn!*
====
Please, explore all the metrics. My recommendation is to access the `/metrics` endpoint in the Data Grid console route. Take a metric that you can monitor through time and execute changes to the cache to check how it varies through time. 

It is important to know the metrics in order to improve performance.

Also, please, check that all the Data Grid metrics have the `domain` tag. This is the tag that you manually added in the `Server Configuration > Basic` section yesterday. Add new tags to see how they are automatically included!
====


[TIP]
====
There are plenty of resources that you can read to get a better understanding of this topic. I recommend this:

* Docs: https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.4/html-single/data_grid_operator_guide/index#monitoring-services[Monitoring Data Grid services].
* Docs: https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.4/html/data_grid_performance_and_sizing_guide/deployment-planning#performance-metric-considerations_deployment-planning[Performance metric considerations].
====






=== 1.3. Openshift Helm Chart.

Helm is a tool to help you define, install, and upgrade applications running on Kubernetes. At its most basic,
Helm is a templating engine that creates Kubernetes manifests. Helm uses a packaging format called
charts. A chart is a collection of files that describe a related set of Kubernetes resources.

You may want to install the Data Grid cluster using Helm Charts due to two main reasons:

* There is already an operator in the latest version and you want to deploy an old version of Data Grid for testing or to prepare for upgrades. The DG operator version 8.4 https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.4/html-single/data_grid_operator_8.4_release_notes/index#rhdg-operator-84GA_rhdg-operator-releases[only supports] deploying Data Grid clusters 8.3.1 and 8.4.x.
* To customize the Data Grid cluster with a specific configuration that is not supported with the Data Grid operator (Please, check if that is supported with the Helm Chart before going to production :) ).

You can install the Helm chart in three quick steps:

.Install the cluster using Helm Charts
[.console-input]
[source, bash]
----
# 1) Add the OpenShift Helm Charts repository.
helm repo add openshift-helm-charts https://charts.openshift.io/

# 2) Customize the values.yaml 

# 3) Install the Helm Chart
helm install cluster-helm openshift-helm-charts/redhat-data-grid
----


.*Now it's your turn!*
====
Please, explore the documentation of the Helm Charts and the values that it accepts.

Check that the previous cluster was deployed successfully.
====





== Level II

=== 2.1. REST API for Caches

Learn how to work with Caches using the REST API and various data formats. 

*First*, you will need to create three caches with similar configuration. Use the knowledge of the section `Cache Configuration` to create three caches with its corresponding encoding format:

* `rest-plain`: Encoding `text/plain`.
* `rest-json`: Encoding `application/json`.
* `rest-proto`: Encoding `application/x-protostream`.

You can use either the Cache CR, the REST API, etc. https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.4/html/data_grid_rest_api/rest_v2_api#rest_v2_create_cache[Here] is how to create a cache using the REST API.

Use the command you learned in the Troubleshooting section to check that the caches are present in the cluster.

TIP: I recommend you use the same configuration for all of them. Use a Distributed cache with two owners. Avoid setting expiration and memory. Just keep it simple! You can use the official documentation for https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.4/html/configuring_data_grid_caches/index[Cache Configuration] and for https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.4/html-single/cache_encoding_and_marshalling/index#cache-encoding[Encoding and Marshalling].

*Second*, you will create entries in all three caches in different formats depending on the cache encoding:

.Put an entry in the cache
[.console-input]
[source, bash]
----
# text/plain
curl -X POST -k -u admin:password -H "Content-Type: text/plain" $RHDG_URL/rest/v2/caches/rest-plain/0 --data "Hello World"

# application/json
curl -X POST -k -u admin:password -H "Content-Type: application/json" $RHDG_URL/rest/v2/caches/rest-json/0 --data '{"Hello": "World", "foo": "bar", "john": "doe"}'

# application/x-protostream
curl -X POST -k -u admin:password -H "Content-Type: text/plain" $RHDG_URL/rest/v2/caches/rest-proto/0 --data "Hello World"
----

*Third*, retrieve the entries also using the REST API. 

.Retrieve entries from the cache
[.console-input]
[source, bash]
----
# text/plain
curl -X GET -k -u admin:password -H "Content-Type: text/plain" $RHDG_URL/rest/v2/caches/rest-plain/0

# application/json
curl -X GET -k -u admin:password -H "Content-Type: application/json" $RHDG_URL/rest/v2/caches/rest-json/0 | jq .
{
  "Hello": "World",
  "foo": "bar",
  "john": "doe"
}

# application/x-protostream
curl -X GET -k -u admin:password -H "Content-Type: text/plain" $RHDG_URL/rest/v2/caches/rest-proto/0 | jq .
{
  "_type": "string",
  "_value": "Hello World"
}
----

IMPORTANT: See how, with a proto cache, you can interact with `text/plain` format in your REST queries, but it then stores the entries in Protostream format. This simplifies interoperability and allows you to consume it as Java Objects directly from a Java Hot Rod client.

Congratulations! Now you are capable of interacting with caches from the REST API to check performance and configuration issues.

.*Now it's your turn!*
====
Please, keep exploring all the capabilities of the REST API. Try to query one of those Caches, retrieve the statistics for every hit and miss, etc.

Also, in general, developers will use Protostream to store complex classes defined with a `.proto` file. Please, check how to create and delete `.proto` files with the REST API. Then, interact with the cache in the same way.
====





=== 2.2 Deploying a Java client





=== 2.3. LDAP-based Authorization 








== Level III


=== 3.1. Datasources with SQL Database.

In-line caching vs side caching.


=== 3.2. Authorization using RBAC.


=== 3.3. Cross-site replication

Finally, we unravel the complexities of cross-site data replication within your cluster. This advanced feature ties together all the previous sections, showcasing how the cumulative knowledge gained contributes to a robust cache optimization strategy.