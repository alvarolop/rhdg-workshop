= Basic Configuration
include::_attributes.adoc[]


== High availability using several replicas

In a production environment, you will always be interested in a high-availability configuration with several replicas, so that restarts of a container in Openshift don't impact the availability of the Data Grid cluster. Enabling it is as simple as modifying the number of replicas of your cluster. You can play with the YAML definition to change the number of replicas. 

First, configure three replicas of your cluster by modifying the following configuration:

[.console-input]
[source, yaml]
----
apiVersion: infinispan.org/v1
kind: Infinispan
metadata:
  name: cluster
spec:
# Here the previous YAML content #
  replicas: 3
----

Wait a few seconds, and you should see a larger list of pods. Execute the following command to monitor the deployment status:

[.console-input]
[source, bash]
----
oc get pods -w 
----

[.console-output]
[source, bash]
----
oc get pods 
NAME                                       READY   STATUS    RESTARTS   AGE
cluster-0                                  1/1     Running   0          21m
cluster-1                                  1/1     Running   0          4m40s
cluster-2                                  1/1     Running   0          4m33s
cluster-config-listener-7db8d897bc-ft59v   1/1     Running   0          21m
----

.Did I Get It Right?
****
Access the logs of all the Data Grid pods to check that they clustered automagically and without errors. You should find logs like these in all the DG pods:

.cluster-2 logs
[source, txt]
----
15:15:09,998 INFO  (main) [org.jgroups.protocols.FD_SOCK2] server listening on *.57800
15:15:10,047 INFO  (main) [org.infinispan.CLUSTER] ISPN000094: Received new cluster view for channel cluster: [cluster-0-34641|2] (3) [cluster-0-34641, cluster-1-35873, cluster-2-12580]
15:15:10,103 INFO  (main) [org.infinispan.CLUSTER] ISPN000079: Channel `cluster` local address is `cluster-2-12580`, physical addresses are `[10.131.1.63:7800]`
# ...
15:15:10,929 INFO  (non-blocking-thread--p2-t1) [org.infinispan.LIFECYCLE] [Context=___hotRodTopologyCache_hotrod-admin][Scope=cluster-2-12580]ISPN100010: Finished rebalance with members [cluster-0-34641, cluster-1-35873, cluster-2-12580], topology id 6
# ...
15:15:10,973 INFO  (main) [org.infinispan.SERVER] ISPN080001: Red Hat Data Grid Server 8.4.3.GA started in 2326ms
----

****

IMPORTANT: Please, before continuing to the next section, scale down the cluster to two replicas to save resources in Openshift. 










== Memory and CPU

Mastering the control of requests and limits will form the cornerstone of your cache management expertise. It is as simple as adding the following configuration to your already running cluster. Please, do not just add this configuration at the end of the YAML, as the operator adds default values for memory. Look for the `container` section and add your values:

[.console-input]
[source, yaml]
----
apiVersion: infinispan.org/v1
kind: Infinispan
metadata:
  name: cluster
spec:
# Here the previous YAML content #
  container:
    cpu: "2000m:100m"
    memory: "1Gi:1Gi"
----

After confirming the new definition of the Infinispan CR, the operator will trigger an ordered restart of the cluster with the new configuration ensuring that there is no loss of service.

NOTE: This configuration is strongly recommended to ensure that the server does not restart due to an OutOfMemory error or the Java Garbage Collector does not have enough resources to perform efficiently.
  







== JVM settings


To wrap up this segment on foundational settings, let's delve into the option of incorporating additional JVM parameters. While you have the freedom to choose any parameter, some common ones include adjusting the timezone, configuring an alternate Garbage Collector, or setting up GC logs. For now, we'll focus on modifying the timezone; later, you can explore the remaining options in the concluding section.


[.console-input]
[source, yaml]
----
apiVersion: infinispan.org/v1
kind: Infinispan
metadata:
  name: cluster
spec:
# Here the previous YAML content #
  container:
    extraJvmOpts: '-Duser.timezone="Europe/Madrid"'
      # Here the MEM and CPU resources #

----



.Did I Get It Right?
****
Access the logs of all the Data Grid pods to check the JVM parameters:

.cluster-0 logs
[source, txt]
----
10:06:21,721 INFO (main) [BOOT] JVM OpenJDK 64-Bit Server VM Red Hat, Inc. 17.0.7+7-LTS
10:06:21,726 INFO (main) [BOOT] JVM arguments = [-server, --add-exports, java.naming/com.sun.jndi.ldap=ALL-UNNAMED, --add-opens, java.base/java.util=ALL-UNNAMED, --add-opens, java.base/java.util.concurrent=ALL-UNNAMED, -Xlog:gc*:file=/opt/infinispan/server/log/gc.log:time,uptimemillis:filecount=5,filesize=3M, -Duser.timezone=Europe/Madrid, -Xmx512m, -XX:+ExitOnOutOfMemoryError, -XX:MetaspaceSize=32m, -XX:MaxMetaspaceSize=96m, -Djava.net.preferIPv4Stack=true, -Djava.awt.headless=true, -Dvisualvm.display.name=redhat-datagrid-server, -Djava.util.logging.manager=org.infinispan.server.loader.LogManager, -Dinfinispan.server.home.path=/opt/infinispan, -classpath, :/opt/infinispan/boot/infinispan-server-runtime-14.0.11.Final-redhat-00001-loader.jar, org.infinispan.server.loader.Loader, org.infinispan.server.Bootstrap, --bind-address=0.0.0.0, -l, /opt/infinispan/server/conf/operator/log4j.xml, -c, operator/infinispan-base.xml, -c, operator/infinispan-admin.xml]
----

As you can see in the middle of the second log line, the `-Duser.timezone=Europe/Madrid` was concatenated to the JVM arguments and the log date/time is now correct.

****





