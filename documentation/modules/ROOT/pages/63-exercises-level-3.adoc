= Exercises: Level III
include::_attributes.adoc[]


== 3.1. Datasources with SQL Database

=== Step 1: Deploy a PostgreSQL database

You already have one PostgreSQL database deployed as a single pod in another namespace: `$YOUR_NAME-db`.

Go to that namespace and check the logs of the pod. Check that there aren't errors and that the logs show the following message:

[.console-output]
[source, log]
----
Starting server...
2023-08-31 10:08:41.603 UTC [1] LOG: redirecting log output to logging collector process
2023-08-31 10:08:41.603 UTC [1] HINT: Future log output will appear in directory "log".
----

This cluster has the following configuration that you will need in Step 3:

* Username: `rhdguser`.
* Password: `password`.
* Database: `datagrid`.



=== Step 2: Download and add the JDBC driver for PostgreSQL

For this exercise, I recommend you use the JDBC driver for PostgreSQL, which is easy to load and configure. You can use JDBC connections with SQL cache stores and JDBC string-based cache stores. It can be downloaded from here: https://jdbc.postgresql.org/download/

We have to instruct RH Data Grid to download the binary and add it to the `server/lib` folder. This can be done easily with the following configuration:

[.console-input]
[source, yaml]
----
apiVersion: infinispan.org/v1
kind: Infinispan
metadata:
  name: cluster
spec:
  dependencies:
    artifacts:
      - maven: ''
        url: https://jdbc.postgresql.org/download/postgresql-42.6.0.jar
----

If everything went fine, you should see the following line in the logs:

[.console-output]
[source, log]
----
12:31:45,410 INFO  (main) [org.infinispan.SERVER] ISPN080027: Loaded extension 'org.postgresql.Driver'
----


.*Now it's your turn!*
[caption=""]
====
If you want to keep investigating this topic, it is possible to add the drivers using a mounted PVC where you previously copied the driver. This way, you don't have any external dependencies during start time. You can try it with the https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.4/html-single/data_grid_operator_guide/index#copying-code_custom-code[official documentation]. 
====

.Check that the JDBC driver was correctly added
[TIP]
====
[.console-input]
[source, bash]
----
# If downloaded using the URL
oc exec -it cluster-0 -c infinispan -- ls /opt/infinispan/server/lib/external-artifacts/lib

# If mounted with the PVC
oc exec -it cluster-0 -c infinispan -- ls /opt/infinispan/server/lib
----
====


=== Step 3: Add the Datasource connector configuration

Data Grid allows you to create managed datasources as part of your Data Grid Server configuration to optimize connection pooling and performance for JDBC database connections. You can then specify the JDNI name of the managed datasources in your caches (Step 4), which centralizes JDBC connection configuration for your deployment.

Add the Connector configuration on the `infinispan.server.datasources` section of the ConfigMap:

[.console-input]
[source, yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-custom-config
data:
  infinispan-config.yaml: >
    infinispan:
      server:
        dataSources:
          - name: ds
            jndiName: 'jdbc/postgres'
            statistics: true
            connectionFactory:
              driver: "org.postgresql.Driver"
              url: "jdbc:postgresql://postgresql.$YOUR_USER-db.svc:5432/datagrid"
              username: "rhdguser"
              password: "password"
            connectionPool:
              initialSize: 1
              maxSize: 10
              minSize: 3
              backgroundValidation: 1000
              idleRemoval: 1
              blockingTimeout: 1000
              leakDetection: 10000
      cache-container:
      # Here the previous contents of the file #
----


.Check that the datasource was correctly added:
[TIP]
====
[.console-input]
[source, bash]
----
curl -s -k -u admin:password $RHDG_URL/rest/v2/server/config | yq -P .
----
====



=== Step 4: Configure the caches to persist data

When you add a managed datasource to Data Grid Server you can add the JNDI name to a JDBC-based cache store configuration. Now, configure the `persistence` section of your cache. JDBC String-Based cache stores, `JdbcStringBasedStore`, use JDBC drivers to load and store values in the underlying database.


[.console-input]
[source, yaml]
----
apiVersion: infinispan.org/v2alpha1
kind: Cache
metadata:
  name: cluster-datasource-cache
spec:
  clusterName: cluster
  name:  datasource-cache
  template: |-
    distributedCache:
      owners: "1"
      mode: SYNC
      statistics: "true"
      encoding:
        key:
          mediaType: application/x-protostream
        value:
          mediaType: application/x-protostream
      persistence:
        stringKeyedJdbcStore:
          dataSource:
            jndi-url: "jdbc/postgres"
          stringKeyedTable:
            prefix: "TBL"
            dropOnExit: true
            createOnStart: true
            idColumn:
              name: "ID"
              type: "VARCHAR(255)"
            dataColumn:
              name: "DATA"
              type: "BYTEA"
            timestampColumn:
              name: "TS"
              type: "BIGINT"
            segmentColumn:
              name: "S"
              type: "INT"
  updates:
    strategy: recreate
----

Now, add a new entry in the cache (For example, using the web console) and check that it is correctly added to the cache and DB:

.Check 
[TIP]
====
[.console-input]
[source, bash]
----
# Check current Databases (Should show "datagrid")
psql postgres://rhdguser:password@localhost:5432/datagrid -c '\l'

# Check cache statistics
curl -s -k -u admin:password $RHDG_URL/rest/v2/caches/datasource-cache | yq -P .

# Check DB table contents
psql postgres://rhdguser:password@localhost:5432/datagrid -c 'SELECT * FROM "TBL_datasource_cache";'
   id   |                                                                          data                                                                          | ts |  s  
--------+--------------------------------------------------------------------------------------------------------------------------------------------------------+----+-----
  8SgFj | \x9801ea078a01430a0b98010b8a01050a034a0164121c9801058a011618ffffffffffffffffff0120ffffffffffffffffff0118ffffffffffffffffff0120ffffffffffffffffff012a00 | -1 | 231
(1 row)
----
====


.*Now it's your turn!*
[caption=""]
====

*Exercise 1*

Do you want to keep learning about this topic? You should learn about Passivation. Passivation configures Data Grid to write entries to cache stores when it evicts those entries from memory. In this way, passivation ensures that only a single copy of an entry is maintained, either in-memory or in a cache store, which prevents unnecessary and potentially expensive writes to persistent storage.

Please, check this https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.4/html-single/configuring_data_grid_caches/index#passivation_persistence[documentation] and configure Passivation in your cache.

*Exercise 2*

On top of that, if you are not interested in Passivation on a DB, but just on a local file, you can configure a Soft-Index File Stores. Check this https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.4/html-single/configuring_data_grid_caches/index#file-stores_persistence[documentation] to configure passivation on file.
====




=== Where can I get more info?


The documentation is split into several documents according to the topic:

* Operator: https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.4/html-single/data_grid_operator_guide/index#deploying-code[Deploying custom code to Data Grid]. This chapter helps you to add the JDBC driver to the Data Grid container during startup.
* Server: https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.4/html-single/data_grid_server_guide/index#managed-datasources[Adding managed datasources to Data Grid Server]: To configure data sources that will then be consumed from caches.
* Caches: https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.4/html/configuring_data_grid_caches/persistence#doc-wrapper[Configuring persistent storage]. This chapter explains the configuration related to configuring persistence and passivation of cache entries.





[#authorization-using-rbac]
== 3.2. Authorization using RBAC


Here we are! We deployed the Data Grid cluster and it's used by several applications. Each of them declares their own caches and stores their information there. At some point, an application wants to store information and prevent other teams from being able to access their data.

In this situation, you have several simpler alternatives, like just deploying a new small cluster for the second application, but Data Grid also provides the possibility of *creating custom roles with reduced permissions* and then configuring *certain caches to only allow certain roles*.

IMPORTANT: You can do all this using RBAC, but to make Exercises independent from each other, this documentation is based on the Properties Realm, not the LDAP Realm. Please revert the configuration that you set up in xref:62-exercises-level-2.adoc#_2_3_ldap_based_authorization[Exercise 2.3 LDAP-based Authorization].


=== The scenario

In the bookshop, we have two applications using Data Grid:

* The *Products* team, which stores the information of the books, the stock information, prices, etc. in different caches.
* The *Discounts* team, which has the discount codes and stores them in a cache.

Each application has two caches and access to the caches should be limited as follows:

* *discounts-01*: `Discounts` has read-write permission and `Products` has read.
* *discounts-02*: `Discounts` has read-write permission and `Products` has none.
* *products-01*: `Products` has read-write permission and `Discounts` has read.
* *products-02*: `Products` has read-write permission and `Discounts` has none.


=== Step 1: Create roles and assign them permissions

First, we need to create new roles that will then determine who can access which cache. Roles are configured at the cluster level, so we will modify the Infinispan CR to create new roles and then assign permissions to them.


.Infinispan CR
[.console-input]
[source, yaml]
----
apiVersion: infinispan.org/v1
kind: Infinispan
metadata:
  name: cluster
spec:
  security:
    authorization:
      enabled: true
      roles:
        - name: products-write
          permissions:
          - ALL_WRITE
          - MONITOR
        - name: products-read
          permissions:
          - ALL_READ
          - MONITOR
        - name: discounts-write
          permissions:
          - ALL_WRITE
          - MONITOR
        - name: discounts-read
          permissions:
          - ALL_READ
          - MONITOR
----


=== Step 2: Assign roles to users

TIP: You already did this in xref:33-server-configuration-security#_security_ii_authorization_with_rbac[Security II: Authorization with RBAC]. The procedure is the same, but using custom roles instead of default roles like `admin`.



=== Step 3: Assign roles to caches

Now, create four new caches with authorization enabled. Please, find an example below:


.Cache CR
[.console-input]
[source, yaml]
----
kind: Cache
apiVersion: infinispan.org/v2alpha1
metadata:
  name: cluster-discounts-01
spec:
  clusterName: cluster
  name: discounts-01
  template: |
    distributedCache:
      encoding:
        key:
          mediaType: application/x-protostream
        value:
          mediaType: application/x-protostream
      memory:
        maxSize: 500MB
        whenFull: REMOVE
      mode: SYNC
      owners: "1"
      security:
        authorization:
          enabled: true
          roles:
          - admin
          - observer
          - discounts-write
          - discounts-read
          - products-read
      statistics: "true"
----

TIP: Please, create the rest of the caches and make sure that you understand how to configure this kind of setting. Take into account that in the Cache CR you do not set permissions, you set roles.



== 3.3. Cross-site replication

Finally, we will set up cross-site data replication. Data Grid can *back up data between clusters running in geographically dispersed data centers and across different cloud providers*. Cross-site replication provides Data Grid with a global cluster view and:

* Guarantees *service continuity* in the event of outages or disasters.
* Presents client applications with a *single point of access* to data in globally distributed caches.

TIP: For evaluation and demonstration purposes, we will configure the Data Grid servers to back up between pods in the same OpenShift cluster (In two different DG clusters). In your production environment, this will probably not be a realistic scenario and you will want to configure cross-site replication between two different OCP clusters.


Cross-site replication is always established at two *levels*. First, we *configure the Data Grid clusters* that will have a global cluster view. This won't sync any data. Then, we configure the *backup cluster where we want to sync data for each cache*. This replication can be Active/Passive or Active/Active.



=== Step 1: Configure the cluster for xsite 

.Infinispan CR
[.console-input]
[source, yaml]
----
apiVersion: infinispan.org/v1
kind: Infinispan
metadata:
  name: cluster
spec:
  service:
    sites:
      local:
        expose:
          type: ClusterIP
        maxRelayNodes: 1
        name: SiteA
      locations:
        - name: SiteB
          clusterName: cluster
          namespace: user1-rhdg-2
----

.Did I Get It Right?
****
Access the logs of the `cluster-0` pod, you should see the following message: 

[source, txt]
----
14:44:14,746 INFO  (jgroups-5,cluster-0-36753) [org.infinispan.XSITE] ISPN000439: Received new cross-site view: [SiteA]
----
****


[IMPORTANT]
====
Be sure to adjust logging categories in your Infinispan CR to decrease log levels for JGroups TCP and RELAY2 protocols. This prevents a large number of log files from using container storage.

[source, yaml]
----
spec:
  logging:
    categories:
      org.jgroups.protocols.TCP: error
      org.jgroups.protocols.relay.RELAY2: error
----
====


Any issues? Check the https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.4/html-single/data_grid_operator_guide/index#configuring-sites-in-clusters_cross-site[official documentation] of cross-site using the operator.



=== Step 2: Create the second cluster


Create a second cluster with a similar configuration as the original cluster but in the second namespace.

Check the logs of the new pod `cluster-router-*` to make sure that there aren't any issues.

TIP: Do not configure the `cache-replicated-XX` caches in the second cluster. You will see that those are not replicated. This is correct. Caches have to be configured independently for each cluster!

.Did I Get It Right?
****
Access the logs of the `cluster-0` pod, you should see the following message: 

[source, txt]
----
14:44:14,746 INFO  (jgroups-5,cluster-0-36753) [org.infinispan.XSITE] ISPN000439: Received new cross-site view: [SiteA, SiteB]
----

Now, both clusters are in the cluster view!
****

Any issues? Check the https://access.redhat.com/webassets/avalon/d/red-hat-data-grid/8.4/html-reports/logs.html[Cross-site replication log messages] documentation.



=== Step 3: Configure a cache backup

Okay, let's configure a cache backup for the `cluster-rest-proto` cache in both clusters. How to do that? Just modify the cache definition *in both clusters* to point to the other site name:


.Cache CR: Backup configuration in cluster A
[.console-input]
[source, yaml]
----
apiVersion: infinispan.org/v2alpha1
kind: Cache
metadata:
  name: cluster-rest-proto
spec:
  template: |-
    distributedCache:
      backups: 
        SiteB: 
          backup: 
            strategy: "SYNC"
            twoPhaseCommit: "false"
----

TIP: Make sure that you are using the site name, not the `clusterName`.


If your configuration is correct, you should see something like this in the web console:

* On top, the web console shows that you are in `Site: SiteA`.
* In the `rest-proto` cache, it says that the cache feature is `Backups`. 

.View the xsite configuration from the OCP web console
image::60-exercises-xsite-dashboard.png[]


=== Step 4: Test it!

Let's make sure that everything is configured correctly. Although you can use the web console, we will show how to check it using the REST API for documentation purposes:

.Retrieve the RHDG URLs
[.console-input]
[source, bash]
----
RHDG_URL_A=$(oc get routes -n user1-rhdg --template='https://{{(index .items 0).spec.host }}')
RHDG_URL_B=$(oc get routes -n user1-rhdg-2 --template='https://{{(index .items 0).spec.host }}')
----

Now, put an entry and retrieve it from the other cluster:

.Perform request
[.console-input]
[source, bash]
----
# PUT the entry in SiteA
curl -X POST -k -u admin:password -H "Content-Type: text/plain" $RHDG_URL_A/rest/v2/caches/rest-proto/0 --data "Hello World"

# GET the entry in SiteA
curl -X GET -k -u admin:password -H "Content-Type: text/plain" $RHDG_URL_A/rest/v2/caches/rest-proto/0

# GET the entry in SiteB
curl -X GET -k -u admin:password -H "Content-Type: text/plain" $RHDG_URL_B/rest/v2/caches/rest-proto/0
----

Great! It worked! We can see the values from both clusters! There's a lot of work to do with cross-site replication if you want to keep investigating this topic, but this is up to you.

Now, continue to the Conclusions section, were we will wrap up all the contents covered in this course.